{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "V_1(s):\n",
      "[[-0.41769113  2.9514063   5.19934689  8.42034327]\n",
      " [ 2.89844461  4.75501819  9.52775666 11.74553166]\n",
      " [ 5.0075909   9.54244782 12.82952442 16.77110887]\n",
      " [ 8.91908285 12.5015446  16.94229861 10.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the grid size\n",
    "grid_size = 4\n",
    "\n",
    "# Initialize rewards\n",
    "rewards = np.full((grid_size, grid_size), -0.1)\n",
    "rewards[2, 1] = -1  # Danger state s3,2\n",
    "rewards[2, 3] = -1  # Danger state s3,4\n",
    "rewards[3, 3] = 10  # Goal state s4,4\n",
    "\n",
    "# Manually set initial value function V_0(s)\n",
    "V = np.zeros((grid_size, grid_size))\n",
    "V[2, 1] = -1.0  # Danger state s3,2\n",
    "V[2, 3] = -1.0  # Danger state s3,4\n",
    "V[3, 3] = 10.0  # Goal state s4,4\n",
    "\n",
    "# Define actions and corresponding transitions\n",
    "actions = {\n",
    "    \"Up\": (-1, 0),\n",
    "    \"Down\": (1, 0),\n",
    "    \"Left\": (0, -1),\n",
    "    \"Right\": (0, 1)\n",
    "}\n",
    "\n",
    "# Transition probabilities\n",
    "prob_intended = 0.7\n",
    "prob_other = 0.1\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Function to get next state\n",
    "def get_next_state(state, action):\n",
    "    return (state[0] + action[0], state[1] + action[1])\n",
    "\n",
    "# Function to check if a state is valid\n",
    "def is_valid_state(state):\n",
    "    return 0 <= state[0] < grid_size and 0 <= state[1] < grid_size\n",
    "\n",
    "# Function to perform value iteration\n",
    "def value_iteration(V, rewards, actions, prob_intended, prob_other, gamma, iterations=1):\n",
    "    V_new = np.copy(V)\n",
    "    for _ in range(iterations):\n",
    "        V_temp = np.copy(V_new)\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                if (i, j) == (3, 3):  # Goal state\n",
    "                    continue  # Value of goal state remains unchanged\n",
    "                values = []\n",
    "                for action_name, action in actions.items():\n",
    "                    value = 0\n",
    "                    next_state = get_next_state((i, j), action)\n",
    "                    if is_valid_state(next_state):\n",
    "                        value += prob_intended * (\n",
    "                            rewards[next_state[0], next_state[1]] + gamma * V_new[next_state[0], next_state[1]]\n",
    "                        )\n",
    "                    else:\n",
    "                        value += prob_intended * (rewards[i, j] + gamma * V_new[i, j])\n",
    "\n",
    "                    for other_action_name, other_action in actions.items():\n",
    "                        if other_action_name != action_name:\n",
    "                            next_state = get_next_state((i, j), other_action)\n",
    "                            if is_valid_state(next_state):\n",
    "                                value += prob_other * (\n",
    "                                    rewards[next_state[0], next_state[1]] + gamma * V_new[next_state[0], next_state[1]]\n",
    "                                )\n",
    "                            else:\n",
    "                                value += prob_other * (rewards[i, j] + gamma * V_new[i, j])\n",
    "\n",
    "                    values.append(value)\n",
    "\n",
    "                V_temp[i, j] = max(values)\n",
    "        V_new = np.copy(V_temp)\n",
    "    return V_new\n",
    "\n",
    "# Perform value iteration for one iteration\n",
    "\n",
    "V_new = value_iteration(V, rewards, actions, prob_intended, prob_other, gamma, iterations=1)\n",
    "\n",
    "# Print the new value function after the iteration\n",
    "print(\"V_1(s):\")\n",
    "print(V_new)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
